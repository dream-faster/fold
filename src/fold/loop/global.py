# Copyright (c) 2022 - Present Myalo UG (haftungbeschr√§nkt) (Mark Aron Szulyovszky, Daniel Szemerey) <info@dreamfaster.ai>. All rights reserved. See LICENSE in root folder.


from typing import Iterable, Optional, Tuple, Union

import pandas as pd

from ..base import Pipeline, TrainedPipelineCard
from ..splitters import Splitter
from ..utils.list import wrap_in_list
from .backend import get_backend
from .checks import check_types_multi_series
from .training import _train_on_window
from .types import BackendType
from .wrap import wrap_transformation_if_needed


def _train_global(
    pipeline: Pipeline,
    data: pd.DataFrame,
    splitter: Splitter,
    backend: Union[BackendType, str] = BackendType.no,
    silent: bool = False,
) -> TrainedPipelineCard:
    """
    Don't use it, it's not tested or ready.
    Trains a global pipeline on multiple time series.
    Currently only supports TrainMethod.sequantial.
    """
    check_types_multi_series(data)
    backend = get_backend(backend)

    pipeline = wrap_in_list(pipeline)
    pipeline = wrap_transformation_if_needed(pipeline)

    splits = splitter.splits(
        length=len(data["unique_id" == data["unique_id"].unique()[0]])
    )

    if len(splits) == 0:
        raise ValueError("No splits were generated by the Splitter.")

    processed_idx = []
    processed_pipelines = []
    processed_pipeline = pipeline

    # TODO:
    #  - initial_fit probably needs to be done on the concatenated data, as some Transformations we can't update, and Optimizers need to select the correct configuration on not just a single time series, but on the whole.
    #  - global models are only possible with tabular models, we should throw an exception if one is passed in that we don't support
    for split in splits:
        for X, y, sample_weights in __get_X_y_from_data(data):
            processed_id, processed_pipeline = _train_on_window(
                processed_pipeline, X, y, sample_weights, split, True, backend, silent
            )
        processed_idx.append(processed_id)
        processed_pipelines.append(processed_pipeline)

    return [
        pd.Series(
            transformation_over_time,
            index=processed_idx,
            name=transformation_over_time[0].name,
        )
        for transformation_over_time in zip(*processed_pipelines)
    ]


def __get_X_y_from_data(
    data: pd.DataFrame,
) -> Iterable[Tuple[pd.DataFrame, pd.Series, Optional[pd.Series]]]:
    unique_ids = data["unique_id"].unique()
    for id in unique_ids:
        data_for_id = data["unique_id" == id]
        data_for_id.index = data_for_id["ds"]
        y = data_for_id["y"]
        X = data_for_id.drop(["y", "unique_id", "ds"], axis="columns")
        sample_weights = (
            data_for_id["sample_weights"]
            if "sample_weights" in data_for_id.columns
            else None
        )
        yield X, y, sample_weights
